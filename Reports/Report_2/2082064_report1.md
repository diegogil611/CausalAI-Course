# **Diego Gil 20182064**

# **Research Question**

The paper aims to address an important question for economists: the search for relevant variables to estimate and infer a phenomenon, studied through high-dimensional and sparse regression models. This means that the phenomenon can be explained by many available and unknown regressors. Belloni A., Chernozhukov V., & Hansen C. (2011) attempt to find a more relevant and parsimonious set of regressors to estimate and infer the model. To find the proper fit among a very large set of control variables, a formal selection method l1or Lasso method is used, which minimizes the least squares criterion by adding a penalty based on the l1norm (the sum of the absolute values of the coefficients multiplied by the penalty parameter λ). This l1penalty reduces many coefficients to zero, which is known as sparse adjustment. In a second stage, greater robustness of the model is obtained by selecting the most relevant variables or non-zero coefficients from the Lasso method. These serve to perform a new OLS regression, improving the precision of estimators with the selected coefficients. This Post-Lasso method is more parsimonious and contains the most relevant variables to explain the phenomenon.

# **Strengths and Weaknesses**

The methodological strength of selecting appropriate variables lies in the statistical process of the penalization condition. The construction of an approximate model is based on the oracle problem. The objectives are to minimize the mean squared errors between the actual values of the function and the approximated values. In addition, the complexity of the model must be reduced to adequately adjust the variance to avoid overfitting.

Robustness tests based on Lasso are performed with different configurations, including the use of instrumental variables. The behavior of noise is examined in situations of noise signals and instruments. The tests show that models based on Lasso, including those specified by cross-validation, perform better in terms of reducing the mean squared error and bias in high-dimensional scenarios, compared to traditional methods like two-stage least squares generalized method of moments, among others. In scenarios where there is no signal from instruments, Lasso methods with cross-validation fit better according to the penalty, verifying the semiparametric efficiency generated by Lasso estimators. It is even possible to build valid and robust confidence intervals for structural parameters, even when identification is weak or there are many instruments. 

Although the oracle method is seen as the ideal reference, it is impossible to perfectly know the relevant variables. Nevertheless, the strengths focus on reducing bias and prediction error in Post-Lasso estimates mentioned earlier. However, they critically depend on the penalty parameter λ. Incorrect selection of this parameter can lead to overfitting or underfitting, affecting the accuracy of estimates and predictions. The use of cross-validation techniques for λ selection can be computationally intensive or may not always provide an optimal solution. As the number of variables increases, iterative methods require more computational resources. Similarly, in cases of overfitting or underfitting, the sparsity of variables may bias the use of relevant variables to explain the phenomenon if it is not contrasted with the theoretical relevance of the most important variables, which can make the model harder to interpret. Even in finite samples or practical applications, theoretical results may not hold. That is, statistical tests can be inaccurate or conservative in finite tests.

It also shows statistical weaknesses, as it can present collinearity and is sensitive to outliers and heteroscedasticity. Lasso models may arbitrarily select one of the most relevant variables in the presence of collinearity. All errors are well-behaved throughout the paper; however, that is in a very controlled and impractical setting. Specifications like Lasso for nonlinear models need to be added.

# **Contribution**

The main contribution of this work lies in the specification of a more parsimonious and robust model with an appropriate or sparse selection of variables. High-dimensional models have many variables that may not be relevant as covariates or instrumental variables. The proper selection of variables is a very important element for research in different contexts, such as economic growth or income, as highlighted in the paper. The efficiency and robustness achieved with these models improve weak identification. The penalization in applied economics studies is exemplified by the Solow-Swan-Ramsey growth theory, which underlies a sparse selection of relevant variables.

Computational resources are continually improving, allowing for iterative searches for an appropriate penalty parameter that better adjusts to the use of the most relevant variables. The work highlights methodological innovations in variable selection techniques. An evident example is the Square-root Lasso and Double Selection methods, which allow for an appropriate selection of relevant covariates in regression models or experimental designs.

# **Next Steps**

An important step forward in the search for more parsimonious estimates is to motivate HDS methods and extend them to other theoretical models to improve the selection of appropriate variables in cases of many covariates in current economic phenomena. This would enhance the estimation of control for structural shocks, among other time series behaviors that require precise variable adjustments. Furthermore, the author emphasizes the need to continue investigating the sparsity assumption and to explore other methods that improve the sub-score approaches presented in the article.
